Task 1: github setup, src folder and building the package
-> Github repository creation
-> Local, crate a folder wher you intend to store the project
-> open Anaconda command prompt pointing to the folder
-> code . to open Visual studio
-> open terminal - command prompt and run the following
-> command: conda create -p venv python==3.8 -y   =>  venv is name of the environment, -y is yes for all installation permissions
-> The above way of setting up is ideal as the required libraries are installedin venv environment 
-> Now activate the environment by using the command: 
    conda activate venv/
-> Now that we activated the environment, we can clone this in the entire directory so that we can commit our code in the Github
-> Now in the environment use the command: git init => to initialize empty git repository
-> Now its time to add a ReadMe file, so let us create one 
-> Add the ReadMe file to the Github using the command: git add README.md
-> No let us commit it to the repository using the command: git commit -m "first commit"
-> Note: use: git status, to see the status of the commit
-> Now before commiting, let us make sure it pointing to the MAIN branch using the command: git branch -M main
-> Now let us add the origin repository by using the command: git remote add origin https://github.com/MeghanaSai74818/MLProject.git
-> Now to see the remote repository associated with this git, use the command: git remote -v
-> Now we can push the file from Origin to Main using the command: git push -u origin main
-> Note: If you are doing it the first time set git global by using the following commands:
    git config --global user.name "MeghanaSai74818"
    git config --global user.email "meghanareddy74818@gmail.com"
-> Now let us create a new file on git called gitignore and set it to python and commit changes so that the files that are not required are ignored
-> Now we need to get the changes that we made on git to local using command: git pull
-> The above process can be automated.
-> On local, create new files setup.py and requirements.txt 
-> setup.py (metadata related to the application) is responsible in creating the machine learning application as a package and also deploy in pypy
-> we will create a new folder in mlproject called src and in that create a file named __init__.py
-> setup.py finds the packages while running find_packsges() by locating the file __init__.py; all the folders containing the file will be considered as packages and it will build them which enables them to be imported like other open packages like seaborn etc
->  we can install the setup.py directly or when we are installing requirements, we should run setup.py to build all the packages so inorder to enable that we add -e . at the end of requirements.txt file to trigger the setup.py
-> Now we can run pip install -r requirements.txt to install the packages => this produces mlproject.egg-info, which indicates the package is installed and can be used when deployed in pypy
-> Now we can add them in the git using command: git add .
-> commit them using the command: git commit -m "environment setup"
-> Pushing to main is done using command: git push -u origin main

TASK 2: Project structure creation, Logging and Exception Handling
-> create components folder in the src folder and add __init__.py file so that it can be imported as a package; components are the modules we will be creating like data ingestion - reading data from sources
-> create a file called data_ingestion.py in components and once we ingest data, we will transform the data so create a file called data_transformation.py 
-> After transorming the data, we will train the model with the data so, create a file called model_trainer.py
-> Now that we have the components in place, we have to create pipeline; create a folder in src called pipeline
-> we can have two types of pipeline, training and prediction; create a file called train_pipeline.py and then we can trigger the components we created to train the model from train_pipeline.py 
-> Also create files called predict_pipeline.py and __init__.py files in the pipeline folder 
-> Now create 3 files called logger.py for logging, exception.py for exception handling and utils.py for any functionalities that are common to entire applicationwill be written in utils.py
-> Now let us start with the exception handling and write custom code for it (or we can get the generic code from the web by searching for exception handling python); whenever we raise a custom exception in a try catch block we use the class written in the exception.py to print the error message
-> Next moving on to logger, any execution that happens then we will log everything so that if any exceptions are encountered then we can solve the issue using logger by logging into text file
-> nOTE: os.getcwd() FETCHES CURRENT WORKING DIRECTORY
-> We can test the logger by running it using the command: python src/logger.py; it will create a new folder and a file in it and starts logging
-> Let us publish the changes using the commands: 
        git add .
        git commit -m "Project Structure"
        git push -u origin main

Task 3 : Data ingestion
-> Here we need to provide input to the ingestion component mostly interms of path as in where to save the train and test data and so on; so, let us create a class called DataIngestionConfig
-> I want to use this DataIngestionConfig, we have to use a decorator @dataclass; normally, inside a class we use init to define the class variable but if we use @dataclass, we can directly define the class variable
-> Note: Use dataclass only for classes wher you just need variables but not functions; if you have functions proceed with __init__
-> Now write a class for data_ingestion and initialize class variable using the above class using ingestion_config
-> here we need to write a function called initiate_data_ingestion, to read data from sources like databases or csv; also add try catch block to catach any exceptions
    -> In try block, read the dataset into a dataframe and also log it; it is important to log often to understand or debug the code
    -> Next, let us create folders to store the data that we require throughout the process and also we can save the df as a csv in the raw_data_path; utilize the ingestion_config.objects to fetch the path
    -> Now let us split the data into train above and test and save the train and test data as csv in the artifact folder
    -> Now return the paths where we stord the test and train data, so that we can access it in data data_transformation
    -> and add the exception block so that it raises a custom exception
-> Now before performing data_ingestion, we need to install scikit-learn; so, add it to the requirements.txt and comment the -e. to avoid building the package again, now run it using
    pip install -r requirements.txt
-> After adding a main function and initializing the data_ingestion, run data ingestion
    python src/components/data_ingestion.py
-> 
-> 
-> 
-> 
-> 
-> 
-> 
-> 
-> 
-> 
-> 
-> 
-> 
-> 
-> 



-> 
